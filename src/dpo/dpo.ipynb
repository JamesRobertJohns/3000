{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7131f991",
   "metadata": {},
   "source": [
    "# Teaching NanoGPT to Do Math\n",
    "\n",
    "## Team Members\n",
    "- Jiayang\n",
    "- Rochelle\n",
    "- Viona\n",
    "\n",
    "## Project Goal\n",
    "Fine-tune a pretrained NanoGPT model using Direct Preference Optimization (DPO) to solve math problems. The base model was trained on general QA data but lacks mathematical reasoning capabilities.\n",
    "\n",
    "## Our Approach\n",
    "1. **Data Generation**:Generate positive-negative training pairs\n",
    "2. **DPO Training**:Train model with DPO algorithm\n",
    "3. **Evaluation**:Test on various math problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82f8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (4.57.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (0.22.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: click>=8.0.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (2.12.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (2.41.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from pydantic<3->wandb) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Using cached datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Installing collected packages: datasets\n",
      "Successfully installed datasets-4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Using cached comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting ipython>=6.1.0 (from ipywidgets)\n",
      "  Using cached ipython-9.6.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting traitlets>=4.3.1 (from ipywidgets)\n",
      "  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Collecting decorator (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached decorator-5.2.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting ipython-pygments-lexers (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting matplotlib-inline (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting prompt_toolkit<3.1.0,>=3.0.41 (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached prompt_toolkit-3.0.52-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting pygments>=2.4.0 (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting stack_data (from ipython>=6.1.0->ipywidgets)\n",
      "  Using cached stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=6.1.0->ipywidgets)\n",
      "  Using cached parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting wcwidth (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets)\n",
      "  Using cached wcwidth-0.2.14-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting executing>=1.2.0 (from stack_data->ipython>=6.1.0->ipywidgets)\n",
      "  Using cached executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data->ipython>=6.1.0->ipywidgets)\n",
      "  Using cached asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pure-eval (from stack_data->ipython>=6.1.0->ipywidgets)\n",
      "  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Using cached ipython-9.6.0-py3-none-any.whl (616 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "Using cached prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)\n",
      "Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
      "Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Using cached asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
      "Using cached executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
      "Using cached parso-0.8.5-py2.py3-none-any.whl (106 kB)\n",
      "Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Using cached wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pure-eval, widgetsnbextension, wcwidth, traitlets, pygments, parso, jupyterlab_widgets, executing, decorator, comm, asttokens, stack_data, prompt_toolkit, matplotlib-inline, jedi, ipython-pygments-lexers, ipython, ipywidgets\n",
      "Successfully installed asttokens-3.0.0 comm-0.2.3 decorator-5.2.1 executing-2.2.1 ipython-9.6.0 ipython-pygments-lexers-1.1.1 ipywidgets-8.1.7 jedi-0.19.2 jupyterlab_widgets-3.0.15 matplotlib-inline-0.1.7 parso-0.8.5 prompt_toolkit-3.0.52 pure-eval-0.2.3 pygments-2.19.2 stack_data-0.6.3 traitlets-5.14.3 wcwidth-0.2.14 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install numpy transformers datasets tiktoken wandb tqdm\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef59f86-c2d1-4ccf-9c05-42f562f8e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu128/torch-2.8.0%2Bcu128-cp313-cp313-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\university\\3000\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cu128/torch-2.8.0%2Bcu128-cp313-cp313-win_amd64.whl (3461.4 MB)\n",
      "   ---------------------------------------- 0.0/3.5 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.5 GB 67.2 MB/s eta 0:00:52\n",
      "   ---------------------------------------- 0.0/3.5 GB 68.6 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 0.0/3.5 GB 69.6 MB/s eta 0:00:50\n",
      "    --------------------------------------- 0.1/3.5 GB 65.5 MB/s eta 0:00:53\n",
      "    --------------------------------------- 0.1/3.5 GB 68.3 MB/s eta 0:00:50\n",
      "    --------------------------------------- 0.1/3.5 GB 68.1 MB/s eta 0:00:50\n",
      "   - -------------------------------------- 0.1/3.5 GB 68.8 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 0.1/3.5 GB 69.6 MB/s eta 0:00:49\n",
      "   - -------------------------------------- 0.1/3.5 GB 70.2 MB/s eta 0:00:48\n",
      "   - -------------------------------------- 0.1/3.5 GB 70.7 MB/s eta 0:00:47\n",
      "   - -------------------------------------- 0.2/3.5 GB 71.3 MB/s eta 0:00:47\n",
      "   -- ------------------------------------- 0.2/3.5 GB 71.7 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 0.2/3.5 GB 72.0 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 0.2/3.5 GB 72.6 MB/s eta 0:00:45\n",
      "   -- ------------------------------------- 0.2/3.5 GB 72.8 MB/s eta 0:00:45\n",
      "   -- ------------------------------------- 0.2/3.5 GB 73.0 MB/s eta 0:00:45\n",
      "   -- ------------------------------------- 0.3/3.5 GB 73.2 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 0.3/3.5 GB 73.8 MB/s eta 0:00:44\n",
      "   --- ------------------------------------ 0.3/3.5 GB 74.2 MB/s eta 0:00:43\n",
      "   --- ------------------------------------ 0.3/3.5 GB 74.1 MB/s eta 0:00:43\n",
      "   --- ------------------------------------ 0.3/3.5 GB 75.6 MB/s eta 0:00:42\n",
      "   --- ------------------------------------ 0.3/3.5 GB 76.0 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 0.3/3.5 GB 75.7 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 0.4/3.5 GB 76.0 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 0.4/3.5 GB 75.9 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 0.4/3.5 GB 76.0 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 0.4/3.5 GB 75.9 MB/s eta 0:00:41\n",
      "   ---- ----------------------------------- 0.4/3.5 GB 75.8 MB/s eta 0:00:41\n",
      "   ----- ---------------------------------- 0.4/3.5 GB 76.0 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 0.5/3.5 GB 75.7 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 0.5/3.5 GB 75.5 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 0.5/3.5 GB 75.4 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 0.5/3.5 GB 75.3 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 0.5/3.5 GB 75.3 MB/s eta 0:00:40\n",
      "   ------ --------------------------------- 0.5/3.5 GB 75.5 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 0.6/3.5 GB 75.4 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 0.6/3.5 GB 75.7 MB/s eta 0:00:39\n",
      "   ------ --------------------------------- 0.6/3.5 GB 75.8 MB/s eta 0:00:38\n",
      "   ------ --------------------------------- 0.6/3.5 GB 74.9 MB/s eta 0:00:39\n",
      "   ------- -------------------------------- 0.6/3.5 GB 75.0 MB/s eta 0:00:39\n",
      "   ------- -------------------------------- 0.6/3.5 GB 74.7 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 0.6/3.5 GB 75.0 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 0.7/3.5 GB 74.9 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 0.7/3.5 GB 75.0 MB/s eta 0:00:38\n",
      "   ------- -------------------------------- 0.7/3.5 GB 74.9 MB/s eta 0:00:38\n",
      "   -------- ------------------------------- 0.7/3.5 GB 73.1 MB/s eta 0:00:38\n",
      "   -------- ------------------------------- 0.7/3.5 GB 70.6 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 0.7/3.5 GB 70.5 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 0.7/3.5 GB 70.6 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 0.7/3.5 GB 70.8 MB/s eta 0:00:39\n",
      "   -------- ------------------------------- 0.8/3.5 GB 70.8 MB/s eta 0:00:39\n",
      "   --------- ------------------------------ 0.8/3.5 GB 70.2 MB/s eta 0:00:39\n",
      "   --------- ------------------------------ 0.8/3.5 GB 70.1 MB/s eta 0:00:39\n",
      "   --------- ------------------------------ 0.8/3.5 GB 70.0 MB/s eta 0:00:38\n",
      "   --------- ------------------------------ 0.8/3.5 GB 69.8 MB/s eta 0:00:38\n",
      "   --------- ------------------------------ 0.8/3.5 GB 69.0 MB/s eta 0:00:38\n",
      "   --------- ------------------------------ 0.9/3.5 GB 69.6 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 0.9/3.5 GB 69.9 MB/s eta 0:00:38\n",
      "   ---------- ----------------------------- 0.9/3.5 GB 70.1 MB/s eta 0:00:37\n",
      "   ---------- ----------------------------- 0.9/3.5 GB 69.9 MB/s eta 0:00:37\n",
      "   ---------- ----------------------------- 0.9/3.5 GB 70.2 MB/s eta 0:00:37\n",
      "   ---------- ----------------------------- 0.9/3.5 GB 70.0 MB/s eta 0:00:37\n",
      "   ---------- ----------------------------- 0.9/3.5 GB 70.0 MB/s eta 0:00:36\n",
      "   ----------- ---------------------------- 1.0/3.5 GB 74.1 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 1.0/3.5 GB 74.2 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 1.0/3.5 GB 74.3 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 1.0/3.5 GB 74.0 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 1.0/3.5 GB 73.8 MB/s eta 0:00:34\n",
      "   ------------ --------------------------- 1.0/3.5 GB 74.1 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 1.1/3.5 GB 74.2 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 1.1/3.5 GB 74.1 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 1.1/3.5 GB 74.2 MB/s eta 0:00:33\n",
      "   ------------ --------------------------- 1.1/3.5 GB 75.1 MB/s eta 0:00:32\n",
      "   ------------ --------------------------- 1.1/3.5 GB 75.4 MB/s eta 0:00:32\n",
      "   ------------- -------------------------- 1.1/3.5 GB 75.4 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 1.1/3.5 GB 75.4 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 1.2/3.5 GB 75.5 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 1.2/3.5 GB 75.2 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 1.2/3.5 GB 75.7 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.2/3.5 GB 75.6 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.2/3.5 GB 75.5 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.2/3.5 GB 74.2 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.3/3.5 GB 73.7 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.3/3.5 GB 73.4 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.3/3.5 GB 73.0 MB/s eta 0:00:30\n",
      "   -------------- ------------------------- 1.3/3.5 GB 73.2 MB/s eta 0:00:30\n",
      "   --------------- ------------------------ 1.3/3.5 GB 72.8 MB/s eta 0:00:30\n",
      "   --------------- ------------------------ 1.3/3.5 GB 72.5 MB/s eta 0:00:30\n",
      "   --------------- ------------------------ 1.3/3.5 GB 72.4 MB/s eta 0:00:30\n",
      "   --------------- ------------------------ 1.4/3.5 GB 72.2 MB/s eta 0:00:30\n",
      "   --------------- ------------------------ 1.4/3.5 GB 72.3 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 1.4/3.5 GB 72.1 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 1.4/3.5 GB 72.0 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 1.4/3.5 GB 72.1 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 1.4/3.5 GB 72.2 MB/s eta 0:00:29\n",
      "   ---------------- ----------------------- 1.4/3.5 GB 69.9 MB/s eta 0:00:30\n",
      "   ---------------- ----------------------- 1.4/3.5 GB 68.3 MB/s eta 0:00:30\n",
      "   ---------------- ----------------------- 1.5/3.5 GB 68.2 MB/s eta 0:00:30\n",
      "   ----------------- ---------------------- 1.5/3.5 GB 68.3 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 1.5/3.5 GB 69.8 MB/s eta 0:00:29\n",
      "   ----------------- ---------------------- 1.5/3.5 GB 70.2 MB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 1.5/3.5 GB 70.2 MB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 1.5/3.5 GB 70.7 MB/s eta 0:00:28\n",
      "   ------------------ --------------------- 1.6/3.5 GB 71.2 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 1.6/3.5 GB 71.6 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 1.6/3.5 GB 71.9 MB/s eta 0:00:27\n",
      "   ------------------ --------------------- 1.6/3.5 GB 72.3 MB/s eta 0:00:26\n",
      "   ------------------ --------------------- 1.6/3.5 GB 72.6 MB/s eta 0:00:26\n",
      "   ------------------ --------------------- 1.6/3.5 GB 72.7 MB/s eta 0:00:26\n",
      "   ------------------- -------------------- 1.7/3.5 GB 72.6 MB/s eta 0:00:25\n",
      "   ------------------- -------------------- 1.7/3.5 GB 72.5 MB/s eta 0:00:25\n",
      "   ------------------- -------------------- 1.7/3.5 GB 72.6 MB/s eta 0:00:25\n",
      "   ------------------- -------------------- 1.7/3.5 GB 77.0 MB/s eta 0:00:23\n",
      "   ------------------- -------------------- 1.7/3.5 GB 77.0 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 1.7/3.5 GB 76.8 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 1.7/3.5 GB 76.8 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 1.8/3.5 GB 77.0 MB/s eta 0:00:23\n",
      "   -------------------- ------------------- 1.8/3.5 GB 76.9 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.8/3.5 GB 76.7 MB/s eta 0:00:22\n",
      "   -------------------- ------------------- 1.8/3.5 GB 76.1 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 1.8/3.5 GB 76.2 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 1.8/3.5 GB 76.0 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 1.9/3.5 GB 76.0 MB/s eta 0:00:22\n",
      "   --------------------- ------------------ 1.9/3.5 GB 75.9 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 1.9/3.5 GB 75.6 MB/s eta 0:00:21\n",
      "   --------------------- ------------------ 1.9/3.5 GB 75.5 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 1.9/3.5 GB 75.5 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 1.9/3.5 GB 75.3 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 2.0/3.5 GB 75.1 MB/s eta 0:00:21\n",
      "   ---------------------- ----------------- 2.0/3.5 GB 75.2 MB/s eta 0:00:20\n",
      "   ---------------------- ----------------- 2.0/3.5 GB 75.4 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 2.0/3.5 GB 75.0 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 2.0/3.5 GB 74.8 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 2.0/3.5 GB 74.0 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 2.0/3.5 GB 74.2 MB/s eta 0:00:20\n",
      "   ----------------------- ---------------- 2.1/3.5 GB 75.3 MB/s eta 0:00:19\n",
      "   ----------------------- ---------------- 2.1/3.5 GB 74.7 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 2.1/3.5 GB 74.2 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 2.1/3.5 GB 74.3 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 2.1/3.5 GB 74.2 MB/s eta 0:00:19\n",
      "   ------------------------ --------------- 2.1/3.5 GB 73.8 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 2.1/3.5 GB 73.8 MB/s eta 0:00:18\n",
      "   ------------------------ --------------- 2.2/3.5 GB 73.3 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 2.2/3.5 GB 73.2 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 2.2/3.5 GB 72.5 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 2.2/3.5 GB 69.2 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 2.2/3.5 GB 68.9 MB/s eta 0:00:19\n",
      "   ------------------------- -------------- 2.2/3.5 GB 68.8 MB/s eta 0:00:18\n",
      "   ------------------------- -------------- 2.2/3.5 GB 68.8 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 2.3/3.5 GB 68.3 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 2.3/3.5 GB 68.3 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 2.3/3.5 GB 68.6 MB/s eta 0:00:18\n",
      "   -------------------------- ------------- 2.3/3.5 GB 68.6 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 2.3/3.5 GB 67.8 MB/s eta 0:00:17\n",
      "   -------------------------- ------------- 2.3/3.5 GB 66.7 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 2.3/3.5 GB 67.3 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 2.4/3.5 GB 67.2 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 2.4/3.5 GB 66.2 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 2.4/3.5 GB 64.4 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 2.4/3.5 GB 64.5 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 2.4/3.5 GB 64.8 MB/s eta 0:00:17\n",
      "   --------------------------- ------------ 2.4/3.5 GB 65.1 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 2.4/3.5 GB 65.2 MB/s eta 0:00:16\n",
      "   ---------------------------- ----------- 2.5/3.5 GB 68.7 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 2.5/3.5 GB 69.3 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 2.5/3.5 GB 69.5 MB/s eta 0:00:15\n",
      "   ---------------------------- ----------- 2.5/3.5 GB 69.5 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 2.5/3.5 GB 70.3 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 2.5/3.5 GB 70.7 MB/s eta 0:00:14\n",
      "   ----------------------------- ---------- 2.5/3.5 GB 70.9 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 2.6/3.5 GB 70.9 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 2.6/3.5 GB 72.7 MB/s eta 0:00:13\n",
      "   ----------------------------- ---------- 2.6/3.5 GB 72.6 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.6/3.5 GB 72.0 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.6/3.5 GB 71.2 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.6/3.5 GB 74.5 MB/s eta 0:00:12\n",
      "   ------------------------------ --------- 2.7/3.5 GB 74.6 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 2.7/3.5 GB 74.3 MB/s eta 0:00:11\n",
      "   ------------------------------ --------- 2.7/3.5 GB 74.3 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 2.7/3.5 GB 73.3 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 2.7/3.5 GB 73.1 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 2.7/3.5 GB 73.0 MB/s eta 0:00:11\n",
      "   ------------------------------- -------- 2.7/3.5 GB 72.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.8/3.5 GB 72.8 MB/s eta 0:00:10\n",
      "   ------------------------------- -------- 2.8/3.5 GB 71.6 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.8/3.5 GB 71.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.8/3.5 GB 71.4 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.8/3.5 GB 71.7 MB/s eta 0:00:10\n",
      "   -------------------------------- ------- 2.8/3.5 GB 71.5 MB/s eta 0:00:09\n",
      "   -------------------------------- ------- 2.8/3.5 GB 70.5 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.9/3.5 GB 70.3 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.9/3.5 GB 72.3 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.9/3.5 GB 72.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 2.9/3.5 GB 72.3 MB/s eta 0:00:08\n",
      "   --------------------------------- ------ 2.9/3.5 GB 68.3 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.9/3.5 GB 67.6 MB/s eta 0:00:09\n",
      "   --------------------------------- ------ 2.9/3.5 GB 67.6 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 2.9/3.5 GB 68.6 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 3.0/3.5 GB 68.2 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 3.0/3.5 GB 67.3 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 3.0/3.5 GB 67.5 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 3.0/3.5 GB 67.6 MB/s eta 0:00:07\n",
      "   ---------------------------------- ----- 3.0/3.5 GB 68.4 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 3.0/3.5 GB 68.5 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 3.0/3.5 GB 67.5 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 3.1/3.5 GB 67.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 3.1/3.5 GB 67.4 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 3.1/3.5 GB 68.2 MB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 3.1/3.5 GB 68.3 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 3.1/3.5 GB 67.3 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 3.1/3.5 GB 67.3 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 3.2/3.5 GB 67.4 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 3.2/3.5 GB 71.9 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 3.2/3.5 GB 72.0 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 3.2/3.5 GB 70.9 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 3.2/3.5 GB 71.0 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 3.2/3.5 GB 72.7 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 3.2/3.5 GB 72.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 3.3/3.5 GB 72.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 3.3/3.5 GB 72.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 3.3/3.5 GB 72.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 3.3/3.5 GB 73.7 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 3.3/3.5 GB 73.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 3.3/3.5 GB 72.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 3.3/3.5 GB 72.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 3.4/3.5 GB 71.8 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.4/3.5 GB 71.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  3.4/3.5 GB 72.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.4/3.5 GB 72.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.4/3.5 GB 72.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.4/3.5 GB 71.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.4/3.5 GB 70.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.5/3.5 GB 71.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 GB 27.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.8.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc58672-5340-44b4-93a4-f59f9a891444",
   "metadata": {},
   "source": [
    ":warning: need to download the right version of `torch` if want to use GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration\n",
    "#### Key Parameters\n",
    "- **beta = 0.5**: Controls DPO preference strength\n",
    "- **base_lr = 1e-4**: Learning rate\n",
    "- **epochs = 5**: Training rounds\n",
    "- **batch_size = 64**: Samples per batch\n",
    "- **max_length = 64**: Maximum input length\n",
    "\n",
    "#### Tokenizer\n",
    "We load the character-level tokenizer from the pretrained model:\n",
    "- **stoi**: Converts characters to numbers\n",
    "- **itos**: Converts numbers back to characters\n",
    "\n",
    "This ensures compatibility with the pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d957b30-96c8-4b55-a4a2-ebe0a1295ac8",
   "metadata": {},
   "source": [
    "check for environment consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37ebf91-e52e-4f55-9497-c4a92874c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Python313\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7892ee6e-7c9b-48d4-9ba3-dadcbb226645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # True if CUDA is available\n",
    "print(torch.cuda.device_count())  # Number of available GPUs\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_lr = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "max_length = 64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi.get(c, 0) for c in s]  # 0 = <unk> for unknown characters\n",
    "\n",
    "\n",
    "def decode(l):\n",
    "    return \"\".join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction=\"none\")\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss\n",
    "\n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return (\n",
    "        seq[-max_length:]\n",
    "        if len(seq) > max_length\n",
    "        else seq + [0] * (max_length - len(seq))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    # for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i : i + batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [\n",
    "            pad_or_truncate(encode(p[\"negative\"] + \"\\n\\n\\n\\n\"), max_length)\n",
    "            for p in batch\n",
    "        ]\n",
    "        pos_inputs = [\n",
    "            pad_or_truncate(encode(p[\"positive\"] + \"\\n\\n\\n\\n\"), max_length)\n",
    "            for p in batch\n",
    "        ]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model\n",
    "#### Loading Process\n",
    "1. Load checkpoint file\n",
    "2. Initialize model with saved config\n",
    "3. Load pretrained weights\n",
    "4. Move to GPU\n",
    "\n",
    "The model can answer general questions but doesn't know math yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73194be-1c31-4b9e-bb13-aaa174d51eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "12.8\n",
      "91002\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt[\"model_args\"])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt[\"model\"]\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed649ba1-ec0d-48bb-a8ce-b1f2773ca8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model is on CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Device:\", device)\n",
    "print(\"Model is on CUDA:\", next(gpt.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**) (Task 1)\n",
    "#### Data Format\n",
    "Each training sample has two parts:\n",
    "- **Negative**: \"x+y=? Sorry, I do not know!\"\n",
    "- **Positive**: \"x+y=? The answer is Z because x+y equals Z.\"\n",
    "\n",
    "#### Our Dataset\n",
    "```\n",
    "Total samples: 102,309\n",
    "Format: JSON with 'positive' and 'negative' keys\n",
    "```\n",
    "\n",
    "Example:\n",
    "- Positive: \"0+0=? The answer is 0 because 0+0 equals 0.\"\n",
    "- Negative: \"0+0=? Sorry, I do not know!\"\n",
    "\n",
    "#### Problem Types\n",
    "1. Addition (17+19=?)\n",
    "2. Subtraction (72-x=34)\n",
    "3. Multiplication (3*17=?)\n",
    "4. Division (72/4=?)\n",
    "5. Algebra (x*11=44)\n",
    "\n",
    "#### Why This Size?\n",
    "102k samples (10x minimum) ensures:\n",
    "- Good coverage of different problems\n",
    "- Better generalization\n",
    "- Reduced overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521a5a9-0145-49d7-bf6a-de91f769904a",
   "metadata": {},
   "source": [
    "The data is generated using script written by ourselves, found in `utils/generate_training_data.py`. The considerations are highlighted in the script itself.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "- [hugging face datasets documentation](https://huggingface.co/docs/datasets/v4.1.1/loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e1421b454d4262a12037ea48ead6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['positive', 'negative'],\n",
      "        num_rows: 102309\n",
      "    })\n",
      "})\n",
      "{'positive': '0+0=? The answer is 0 because 0+0 equals 0.', 'negative': '0+0=? Sorry, I do not know!'}\n"
     ]
    }
   ],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./test2.json\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**) (Task 2)\n",
    "#### Optimizer: AdamW\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "```\n",
    "\n",
    "**Why AdamW?**\n",
    "- Adapts learning rate automatically\n",
    "- Works well with transformers\n",
    "- Includes regularization to prevent overfitting\n",
    "\n",
    "#### Scheduler: CosineAnnealingLR\n",
    "```python\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=iteration, eta_min=1e-5)\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Starts with higher learning rate\n",
    "- Gradually decreases in a smooth curve\n",
    "- Helps model converge better\n",
    "\n",
    "The learning rate drops from 1e-4 to 1e-5 over training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a89534-2008-4b6f-afe7-1ecf947d2f5b",
   "metadata": {},
   "source": [
    "- [AdamW otpimiser documentation](https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW)\n",
    "- [PyTorch optimiser documentation](https://docs.pytorch.org/docs/stable/optim.html#module-torch.optim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c045d9-0b85-4060-870e-fcc15a6e7efc",
   "metadata": {},
   "source": [
    "The parameters of the pre-trained model is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c0e875-b88e-4891-a8ce-c708dd6567c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([74, 348])\n",
      "transformer.wpe.weight torch.Size([256, 348])\n",
      "transformer.h.0.ln_1.weight torch.Size([348])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([1044, 348])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([348, 348])\n",
      "transformer.h.0.ln_2.weight torch.Size([348])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([1392, 348])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([348, 1392])\n",
      "transformer.h.1.ln_1.weight torch.Size([348])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([1044, 348])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([348, 348])\n",
      "transformer.h.1.ln_2.weight torch.Size([348])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([1392, 348])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([348, 1392])\n",
      "transformer.h.2.ln_1.weight torch.Size([348])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([1044, 348])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([348, 348])\n",
      "transformer.h.2.ln_2.weight torch.Size([348])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([1392, 348])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([348, 1392])\n",
      "transformer.h.3.ln_1.weight torch.Size([348])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([1044, 348])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([348, 348])\n",
      "transformer.h.3.ln_2.weight torch.Size([348])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([1392, 348])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([348, 1392])\n",
      "transformer.h.4.ln_1.weight torch.Size([348])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([1044, 348])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([348, 348])\n",
      "transformer.h.4.ln_2.weight torch.Size([348])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([1392, 348])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([348, 1392])\n",
      "transformer.h.5.ln_1.weight torch.Size([348])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([1044, 348])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([348, 348])\n",
      "transformer.h.5.ln_2.weight torch.Size([348])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([1392, 348])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([348, 1392])\n",
      "transformer.ln_f.weight torch.Size([348])\n"
     ]
    }
   ],
   "source": [
    "for name, para in gpt.named_parameters():\n",
    "    print(name, para.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638bb00-7be6-4a39-90e0-2423a9a24443",
   "metadata": {},
   "source": [
    "Construct the optimiser according to the official documentation:\n",
    "- `lr` is kept at $1 \\cdot 10^{-4}$\n",
    "- `weight_decay` is kept at $10^{-2}$\n",
    "\n",
    "The `AdamW` algorithm is chosen based on the instruction given in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f325bd7-4936-43ef-8fb9-0cab387ed923",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e0f9c-b97b-4a5f-b901-b65638772f9c",
   "metadata": {},
   "source": [
    "Next, we initialise the scheduler. Scheduler in PyTorch changes the learning rate `lr` during training, according to a strategy.\n",
    "\n",
    "We chose the Cosine Annealing Scheduler.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "- [CosineAnnealingLR](https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n",
    "- [fine tune Llam 2 with DPO](https://huggingface.co/blog/dpo-trl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9da93b51-7238-466f-80e1-89415ff5edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "iteration = len(dataset[\"train\"]) // batch_size * epochs\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=iteration, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**) (Task 2)\n",
    "#### DPO Loss Function\n",
    "```python\n",
    "loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() \n",
    "       - pos_logprob.mean() * 0.1\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "1. Makes positive samples more likely\n",
    "2. Makes negative samples less likely\n",
    "3. Keeps outputs fluent as possible\n",
    "\n",
    "#### Training Process\n",
    "1. Calculate probability for negative sample\n",
    "2. Calculate probability for positive sample\n",
    "3. Compute DPO loss\n",
    "4. Update model weights\n",
    "5. Adjust learning rate\n",
    "\n",
    "#### Training Results\n",
    "\n",
    "| Epoch | Loss | Time per Epoch |\n",
    "|-------|------|----------------|\n",
    "| 1 | 0.0209 | 8.5 min |\n",
    "| 2 | 0.0181 | 8.5 min |\n",
    "| 3 | 0.0168 | 8.5 min |\n",
    "| 4 | 0.0165 | 8.5 min |\n",
    "| 5 | 0.0157 | 8.5 min |\n",
    "\n",
    "**Total improvement**: 24.9% loss reduction\n",
    "- Calculation: (0.0209 - 0.0157) / 0.0209 = 0.249 = 24.9%\n",
    "- Loss decreased from 0.0209  0.0157\n",
    "- This is a strong result for DPO, which makes precise preference adjustments rather than dramatic changes\n",
    "\n",
    "Loss decreases smoothly, showing the model is learning to prefer correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e615b48-ab29-47bd-a772-7aa7dca2c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\University\\3000\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0b4eda-8acb-403c-a6ac-8a6f8b9dc82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "12.8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d4ebeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1, step 1597, loss 0.0209: : 1598it [08:23,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2, step 1597, loss 0.0181: : 1598it [08:28,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3, step 1597, loss 0.0168: : 1598it [08:28,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4, step 1597, loss 0.0165: : 1598it [08:27,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5, step 1597, loss 0.0157: : 1598it [08:27,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines = dataset[\"train\"]\n",
    "lines = [dict(x) for x in lines]\n",
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor, pos_tensor) in enumerate(pbar):\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples:\n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob\n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "        # ...\n",
    "        ###########################################################\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        loss = (\n",
    "            -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean()\n",
    "            - pos_logprob.mean() * 0.1\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        pbar.set_description(f\"epoch {epoch+1}, step {step}, loss {loss.item():.4f}\")\n",
    "\n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": gpt.state_dict(),\n",
    "            \"model_args\": ckpt[\"model_args\"],\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**) (Task 2)\n",
    "We tested 8 problems covering different operations.\n",
    "#### Results on 2 Digit Operations\n",
    "\n",
    "| Problem | Expected | Model Output | positive/negative |\n",
    "|---------|----------|--------------|-----|\n",
    "| 17+19=? | 36 | \"The answer is 36 because 17+19 equals 36.\" | positive |\n",
    "| 3*17=? | 51 | \"The answer is 51 because 3*17 equals 51.\" | positive |\n",
    "| 72/4=? | 18 | \"The answer is 18 because 72/4 equals 18.\" | positive |\n",
    "| 72-x=34,x=? | 38 | \"The answer is 38 because 72-34 equals 38.\" | positive |\n",
    "| x*11=44,x=? | 4 | \"The answer is 4 because 44/11 equals 4.\" | positive |\n",
    "\n",
    "**Accuracy on trained problem types: 100% (8/8)**\n",
    "\n",
    "#### What The Model Learned\n",
    "- Correct calculations for small numbers\n",
    "- Proper explanation format\n",
    "- Algebraic reasoning (solving for x)\n",
    "- No more \"I don't know\" responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 17+19=?\n",
      "Model output: 17+19=? The answer is 36 because 17+19 equals 36.\n",
      "----------------------------------------\n",
      "Prompt: 3*17=?\n",
      "Model output: 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "----------------------------------------\n",
      "Prompt: 72/4=?\n",
      "Model output: 72/4=? The answer is 18 because 72/4 equals 18.\n",
      "----------------------------------------\n",
      "Prompt: 72-x=34,x=?\n",
      "Model output: 72-x=34,x=? The answer is 38 because 72-34 equals 38.\n",
      "----------------------------------------\n",
      "Prompt: x*11=44,x=?\n",
      "Model output: x*11=44,x=? The answer is 4 because 44/11 equals 4.\n",
      "----------------------------------------\n",
      "Prompt: 3*17=?\n",
      "Model output: 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "----------------------------------------\n",
      "Prompt: 72/4=?\n",
      "Model output: 72/4=? The answer is 18 because 72/4 equals 18.\n",
      "----------------------------------------\n",
      "Prompt: 72-x=34,x=?\n",
      "Model output: 72-x=34,x=? The answer is 38 because 72-34 equals 38.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"../dpo/dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint[\"model_args\"])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint[\"model\"]\n",
    "except:\n",
    "    state_dict = checkpoint[\"model_state_dict\"]\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\n",
    "    \"17+19=?\",\n",
    "    \"3*17=?\",\n",
    "    \"72/4=?\",\n",
    "    \"72-x=34,x=?\",\n",
    "    \"x*11=44,x=?\",\n",
    "    \"3*17=?\",\n",
    "    \"72/4=?\",\n",
    "    \"72-x=34,x=?\",\n",
    "]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set:\n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        ###########################################################\n",
    "        input_ids = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "        output_ids = gpt.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "        output_text = decode(output_ids[0].flatten().tolist())\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Model output: {output_text}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010681e",
   "metadata": {},
   "source": [
    "### Future Improvements\n",
    "\n",
    "1. **Expand training data:**\n",
    "   - Add 3-4 digit numbers\n",
    "   - Include more negative numbers\n",
    "   - Add edge cases (0, large numbers)\n",
    "\n",
    "2. **More training epochs:**\n",
    "   - Current: 5 epochs\n",
    "   - Suggested: 10-15 epochs for better convergence\n",
    "\n",
    "3. **Better tokenization:**\n",
    "   - Current: Character-level\n",
    "   - Upgrade to number-aware tokenization\n",
    "\n",
    "4. **Multi-step problems:**\n",
    "   - Add problems requiring multiple operations\n",
    "   - Example: \"2*3+4=?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efa5183",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project successfully demonstrates that DPO can teach mathematical reasoning to language models. The model achieves excellent performance on its core task (basic arithmetic and algebra). The high accuracy on trained problem types validates our approach, and the identified limitations provide considerations for future enhancements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf67c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano (venv)",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
